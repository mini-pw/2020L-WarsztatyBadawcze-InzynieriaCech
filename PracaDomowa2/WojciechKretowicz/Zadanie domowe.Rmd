---
title: "Zadanie domowe 1"
author: "Wojciech Kretowicz"
date: "17 kwietnia 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(PRROC)
library(ranger)

source("Zadanie domowe 1.R")
```

# Glance at the data

```{r, echo=FALSE}
data = OpenML::getOMLDataSet(38)$data
data$Class = as.numeric(data$Class)-1
train = read.csv2("https://raw.githubusercontent.com/mini-pw/2020L-WarsztatyBadawcze-InzynieriaCech/master/PracaDomowa1/indeksy_treningowe.txt",
                  sep = ' ')[,1]

data_train = data[train,]
data_test = data[-train,]
```

```{r}
par(mfrow=c(2,2))
hist(data$TSH)
hist(data$T3)
hist(data$TT4)
hist(data$FTI)
```

# Preprocessing

## Laboratory tests

First, I checked laboratory tests to find out, what are possible values of hormones in a human body. Taking very large margin, I chose following values:

* $TSH < 100$
* $T3 < 30$
* $TT4 < 400$
* $FTI < 400$

## Distributions

Furthermore, I transformed all numerical values with boxcox trasformation, resulting in these distributions:

```{r, echo=FALSE}
data_train = data[train,]
data_test = data[-train,]

data_train = preprocess(data_train)
data_test = preprocess(data_test)
```

```{r}
par(mfrow=c(2,2))
hist(data_train$TSH)
hist(data_train$T3)
hist(data_train$TT4)
hist(data_train$FTI)
```


## Missing values

At this moment missing values and features with suffix "measured" are negligible - there are only 2 features with missing values with aroung 3% of missing values. I used mice package with 'pmm' method to fill these. Them I dropped all features with suffix 'measure'.

## Black box

I have constructed black box with "ranger" to have a comparison and explained it to make accumulated dependency plot and feature importance. It showed me the most important variables in this task.

```{r, echo=FALSE}
data_train_factor = data_train
data_train_factor$Class = as.factor(data_train_factor$Class)

data_test_factor = data_test
data_test_factor$Class = as.factor(data_test_factor$Class)

train_task = makeClassifTask(data=data_train_factor, target='Class')
test_task = makeClassifTask(data=data_test_factor, target='Class')
```


```{r}
library(DALEX)
library(mlr)
black_box = mlr::makeLearner('classif.ranger', predict.type = 'prob')
```

```{r, echo=FALSE}
set.seed(77)
trained = train(black_box, train_task)
probs = predict(trained, test_task)$data$prob.1

fg = probs[data_test$Class == 1]
bg = probs[data_test$Class == 0]
```

### Results

On 5-cv:

* AUPRC: $95.9 +- 1.4$
* AUROC: $99.5 +- 2.5$

On test data:

* AUPRC: $98.5$
* AUROC: $84.0$

```{r}
roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(roc)

set.seed(77)

pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(pr)
```

### Explaining black box

```{r, echo=FALSE}
black_box = ranger(Class~., data=data_train)
```


```{r}
exp = explain(black_box, data_train[,-25],data_train$Class, verbose=FALSE)
library(ingredients)
acc = ingredients::accumulated_dependency(exp, variables = c("age", "sex", "on_thyroxine", "query_on_thyroxine", "on_antithyroid_medication", "pregnant", "thyroid_surgery", "I131_treatment", "query_hypothyroid",         "query_hyperthyroid", "lithium", "goitre", "tumor", "psych", "TSH", "T3", "TT4", "T4U", "FTI"))
plot(acc)
```

```{r}
fi = ingredients::feature_importance(exp)
plot(fi)
```

## Model

I have chosen logistic regression and rpart for modelling, because both are easily interpretable. However, rpart had much better results.

## Feature engineering

I added few more features based on cross val scores of auprc and black box explanation:

* $T3^2$
* $T4U^2$
* $TT4^2$
* $T3/FTI$

## Tuning

I used random search for tuning rpart.

## Results

After all I have got on 5-cv:

* AUPRC: $92.9 +- 4.1$
* AUROC: $96.2 +- 2.9$

And on the test set:

* AUPRC: $91.3$
* AUROC: $49.1$?

```{r, echo=FALSE}
data_train_mod$Class = as.factor(data_train_mod$Class)
task = mlr::makeClassifTask(data=data_train_mod, target = 'Class')
model = mlr::makeLearner('classif.rpart', predict.type = 'prob')
params = makeParamSet( 
  makeDiscreteParam("minsplit", values=seq(5,10,1)), makeDiscreteParam("minbucket", values=seq(round(5/3,0), round(10/3,0), 1)), 
  makeNumericParam("cp", lower = 0.01, upper = 0.05), makeDiscreteParam("maxcompete", values=6), makeDiscreteParam("usesurrogate", values=0), makeDiscreteParam("maxdepth", values=10) )
ctrl = makeTuneControlGrid()
rdesc = makeResampleDesc("CV", iters = 5L, stratify=TRUE)
set.seed(77) 
dt_tuneparam <- tuneParams(learner=model, 
                            resampling=rdesc, 
                            measures=list(tpr,auc, fnr, mmce, tnr, setAggregation(tpr, test.sd)), 
                            par.set=params, 
                            control=ctrl, 
                            task=task, 
                            show.info = FALSE)

dt_tuneparam$x
model = mlr::makeLearner('classif.rpart', predict.type = 'prob', par.vals = dt_tuneparam$x)
model = train(model, task)
data_test_mod$Class = as.factor(data_test_mod$Class)
task_test = mlr::makeClassifTask(data=data_test_mod, target = 'Class')
probs = predict(model, task)$data$prob.1
fg = probs[data_test$Class == 0]
bg = probs[data_test$Class == 1]
```

```{r}
# ROC Curve    
roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(roc)

# PR Curve
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
plot(pr)
```


# Conclusions

Final model, `rpart` is worse than `ranger` but also achieves very good results and not much different. `rpart` is on average 3 points worse on `AUPRC` and 3 points worse on `AUROC`. However, `rpart` is easily understandable by humans. This difference in this case in negligible.
