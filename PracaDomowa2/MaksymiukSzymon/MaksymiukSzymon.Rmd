---
title: "Sick dataset analysis Part 2"
author: "Szymon Maksymiuk"
date: "28 04 2020"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: true
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(OpenML)
library(tidyverse)
library(kableExtra)
library(visdat)
library(naniar)
library(DataExplorer)

```

# Preprocessing

I have decided to omit EDA and textual explanations for actions during preprocessing. This is a continuation of home work 1. I assume it is no longer necessary. As a reminder code of that preprocessing is presented below.

```{r data}
dataset_openml <- getOMLDataSet(data.id = 38)
dataset_raw <- dataset_openml$data
test_index <- read.csv("index.txt", sep = " ")$x
dataset_raw <- dataset_raw[,-which(names(dataset_raw)=="TBG")]
dataset_raw <- dataset_raw[,-which(names(dataset_raw)=="TBG_measured")]
dataset_raw <- dataset_raw[,-which(names(dataset_raw)=="hypopituitary")]
for (i in 1:ncol(dataset_raw)) {
  if (!is.null(levels(dataset_raw[[i]]))) {
    if (all(levels(dataset_raw[[i]]) %in% c("f", "t"))) {
      dataset_raw[i] <- as.numeric(ifelse(dataset_raw[i] == 't', 1, 0))
    }   
  }
}
dataset_raw$sex <- ifelse(dataset_raw$sex == "M", 1, 0)
dataset_raw$Class <- as.factor(ifelse(dataset_raw$Class == "sick", 1, 0))
dataset_raw[dataset_raw$age>200,]
dataset_raw <- dataset_raw[-1365,]
dataset_test <- dataset_raw[-test_index,]
dataset_train <- dataset_raw[test_index,]
```

We handle missing values in observations in the same way as last time. `mice` package will be used along with `pmm` method to impute missing values.

```{r}
library(mice)
m <- mice(dataset_raw, method = "pmm", printFlag = FALSE)
dataset_raw <- complete(m, 3)
dataset_test <- dataset_raw[-test_index,]
dataset <- dataset_raw[test_index,]
dataset <- dataset[-3016,]
```

# Modeling

Once we have our preprocessed data, it is hight time to continue modeling part. `mlr` framework for modeling will be used once again. Last time the decision tree model turned up to be the best, therefore we surely should use a random forest model. Taking all into consideration given models will be presented in this work: 

* random forest,
* gradient boosting machine,
* xgBoost,
* support vector machine.

Similarly to last time, we will train hyperparameters using AUPRC as the target measure.

AUPRC of decision tree from the last homework stands as our gold standard. It equals approximately 0.91.

## Prep

```{r}
library("mlr")
library("auprc")
library("PRROC")
library("mlrMBO")

my_auprc_fun = function(task, model, pred, feats, extra.args) {
  prob <- pred$data$prob.1
  y_truth <- getPredictionTruth(pred)
  

  fg <- prob[y_truth == 1]
  bg <- prob[y_truth == 0]

  pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
  auprc <- pr$auc.integral
  auprc
}

my_auprc = makeMeasure(
  id = "myauprc", name = "AUPRC",
  properties = c("classif", "classif.multi", "req.pred", "req.truth"),
  minimize = FALSE, best = 1, worst = 0,
  fun = my_auprc_fun
)

task <- makeClassifTask(data = dataset, target = "Class")
```

## Random Forest
```{r eval=FALSE}
lrn_tree <- makeLearner("classif.ranger", predict.type = "prob")

par.set <- makeParamSet(
  makeIntegerParam("num.trees", 300, 700),
  makeIntegerParam("mtry", 2, 7),
  makeIntegerParam("min.node.size", 5, 20)
)

cv <- makeResampleDesc("CV", iters = 5L)
ctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(ctrl, iters = 100)
tune_ctrl <- makeTuneControlMBO(mbo.control = ctrl)
res_tree <- tuneParams(lrn_tree, task, cv, par.set = par.set, control = tune_ctrl, measures = my_auprc)

# Best Params: num.trees=300; mtry=7; min.node.size=5
```
Best AUPRC during tuning was $0.9657230$.

## GBM

```{r eval = FALSE}
lrn_tree <- makeLearner("classif.gbm", predict.type = "prob", par.vals = list(distribution = "bernoulli"))

par.set <- makeParamSet(
  makeIntegerParam("n.trees", 300, 700),
  makeIntegerParam("interaction.depth", 1, 5),
  makeNumericParam("shrinkage", -10, -2.5, trafo = function(x) 2^x),
  makeIntegerParam("n.minobsinnode", 5, 20)
)
cv <- makeResampleDesc("CV", iters = 5L)
ctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(ctrl, iters = 100)
tune_ctrl <- makeTuneControlMBO(mbo.control = ctrl)
res_tree_gbm <- tuneParams(lrn_tree, task, cv, par.set = par.set, control = tune_ctrl, measures = my_auprc)

# Best Params: num.trees=300; mtry=7; min.node.size=5
```
Best AUPRC during tuning was $0.9554210$.


## xgBoost

```{r eval = FALSE}
lrn_tree <- makeLearner("classif.xgboost", predict.type = "prob")

par.set <- makeParamSet(
  makeIntegerParam("max_depth", 4, 12),
  makeNumericParam("gamma", 0, 3),
  makeNumericParam("eta", -4, -1, trafo = function(x) 2^x),
  makeNumericParam("lambda", 0, 3)
)
cv <- makeResampleDesc("CV", iters = 5L)
ctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(ctrl, iters = 100)
tune_ctrl <- makeTuneControlMBO(mbo.control = ctrl)
res_tree_xgboost <- tuneParams(lrn_tree, task, cv, par.set = par.set, control = tune_ctrl, measures = my_auprc)

# Best Params: max_depth=11; gamma=0.282; eta=0.0768; lambda=2.13e-05
```
Best AUPRC during tuning was $0.9392885$.

## svm

```{r eval = FALSE}
lrn_tree <- makeLearner("classif.svm", predict.type = "prob")

par.set <- makeParamSet(
  makeNumericParam("cost", 0, 4),
  makeNumericParam("nu", -4, 2, trafo = function(x) 2^x),
  makeNumericParam("gamma", 0, 4)
)
cv <- makeResampleDesc("CV", iters = 5L)
ctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(ctrl, iters = 100)
tune_ctrl <- makeTuneControlMBO(mbo.control = ctrl)
res_tree_svm <- tuneParams(lrn_tree, task, cv, par.set = par.set, control = tune_ctrl, measures = my_auprc)

# Best Params: cost=4; nu=0.0779; gamma=0.169
```
Best AUPRC during tuning was $0.8266146$.

## Summary

As we it turns up that best model is random forest implemented in `ranger` library. It is not a coinsidance because forest type models are quite good for imbalanced data.
# Final model
```{r}
lrn_tree <- makeLearner("classif.ranger", predict.type = "prob", par.vals = list(num.trees=300, 
                                                                                 mtry=7, 
                                                                                 min.node.size=5))

model_tree <- train(lrn_tree, task)

p_tree <- predict(model_tree, newdata = dataset_test)

performance(p_tree, list(auc, my_auprc))
```
